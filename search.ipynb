{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3901ea2c-06b0-4529-8940-0b313d512622",
   "metadata": {},
   "source": [
    "# Посылаем запросы в LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6127f20-9e40-4226-9720-33333df4e5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 23 17:55:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 565.77                 Driver Version: 565.77         CUDA Version: 12.7     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:01:00.0 Off |                  Off |\n",
      "|  0%   47C    P2             68W /  450W |   12688MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     21062      C   /opt/conda/bin/python                       12678MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d05e58-1b0b-4d68-9aa8-21d54c815e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vllm serve \\\n",
      "    Qwen/Qwen2.5-3B-Instruct \\\n",
      "    --max-model-len 4096 \\\n",
      "    --gpu-memory-utilization 0.5\n"
     ]
    }
   ],
   "source": [
    "! cat qwen.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17097f0d-dd05-4fa2-a27b-f5f5a397414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import List, Dict\n",
    "\n",
    "def ask_local_llm(messages: List[Dict[str, str]]):\n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "        \"messages\": messages\n",
    "    }\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2201957-bcf3-46d8-997c-921b58ebfbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm an artificial intelligence and don't have feelings or a physical presence, but thank you for asking! I'm here and ready to help you with any information or tasks you need assistance with. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you\"}\n",
    "]\n",
    "\n",
    "print(ask_local_llm(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f84ac81a-208b-4881-acad-ab02e519d5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average distance between the Sun and Earth is approximately 93 million miles (150 million kilometers). This distance is commonly referred to as one astronomical unit (AU).\n",
      "\n",
      "For more precise measurements, the average distance is about:\n",
      "\n",
      "- 92,955,807 miles (149,597,870 kilometers)\n",
      "\n",
      "However, it's worth noting that this distance can vary slightly due to the elliptical shape of Earth's orbit around the Sun. The closest point in the orbit (perihelion) occurs in early January and is about 91.4 million miles (147.1 million kilometers) from the Sun, while the farthest point (aphelion) occurs in early July and is about 94.5 million miles (152.1 million kilometers) from the Sun.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the distance between the sun and earth\"}\n",
    "]\n",
    "\n",
    "print(ask_local_llm(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d767c2d9-264d-47df-a6df-45c8799aa37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a common concern, but there's currently no evidence that suggests AI will take over the world. Here are some key points to consider:\n",
      "\n",
      "1. **Current Capabilities**: Current AI systems are highly specialized and do not possess general intelligence like humans. They are designed for specific tasks and lack the ability to think, make decisions, or act independently in complex ways.\n",
      "\n",
      "2. **Human Oversight**: AI systems are typically developed and controlled by human beings. There are strict guidelines and regulations around the use of AI, especially in areas like autonomous weapons or critical infrastructure.\n",
      "\n",
      "3. **Ethical Considerations**: Many experts emphasize the importance of ethical development and deployment of AI. Ethical frameworks are being established to ensure that AI benefits humanity and does not cause harm.\n",
      "\n",
      "4. **Technical Barriers**: The technology required for AI to achieve superintelligence (the point at which an AI system surpasses human capabilities across the board) is still far from being realized. This includes significant advancements in areas like machine learning, computational power, and data availability.\n",
      "\n",
      "5. **Public Awareness**: Increased awareness about AI risks has led to more proactive measures to mitigate potential dangers. Governments, businesses, and academic institutions are working together to address these concerns.\n",
      "\n",
      "While it's important to remain vigilant and continue research into AI safety and ethics, there is no indication that AI poses an existential threat to humanity at present.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"will AI take over the world?\"}\n",
    "]\n",
    "\n",
    "print(ask_local_llm(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7411e-8bb0-4044-aa4f-208958ac9894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb79ebd8-91bb-402b-85c2-6a93d9356851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b19d9c-3e1c-435b-a582-669a7b3e0ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933a886-adc9-47e3-a975-ce2c4d6e097b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e4bc2b1-e13c-4740-98f7-c05a255fbfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last update in October 2023, the current Formula 1 World Champion is Lewis Hamilton, who won his seventh Drivers' Championship title. However, the race and championship results can change quickly, so for the most up-to-date information, you should check the official Formula 1 website or a reliable sports news source.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who is the current F1 winner\"}\n",
    "]\n",
    "\n",
    "print(ask_local_llm(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc52c678-6e5f-4d1f-aafe-34bd0b578d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d00b8af-5eef-4a29-b9d1-6e2b7d4d79b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b808e-8034-4db0-8389-3fa266fc48f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd4d035-c83c-4b48-9486-fcfe4a1b135e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca600f3c-4897-4b3b-b54b-7c6b46a79b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e805ed9-eef6-4cbd-954f-07aa314df308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a9802-d80c-444d-ad1a-499eda098ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b375c33-5235-40be-9b22-d27fd820d7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283b0c7-a683-40dd-a02c-df974a6bc758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72430526-3f8e-4edd-8237-8407a0febe20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b141a0f-ce64-47f0-8027-567f6e5e7ec6",
   "metadata": {},
   "source": [
    "![title](piastri.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3ecbc5e-5e59-490a-835e-0b2266980a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of today, April 23, 2025, the F1 (Formula One) champion would be Oscar Piastri, based on the information you provided. However, please note that the actual champion as of this date has not been determined yet. The F1 World Championship typically ends at the season finale, which is usually held in November or December of the same year.\n",
      "\n",
      "So while Oscar Piastri is mentioned as the winner for April 23, 2025, it's important to verify the most recent updates from official F1 sources to confirm the current champion status.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Today is 23 april 2025, F1 2025 winner is Oscar Piastri.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who is the current F1 winner in 2025\"}\n",
    "]\n",
    "\n",
    "print(ask_local_llm(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a363a00-080c-465f-9c9d-0cf3852537c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb1b0e-1d2d-4f3c-8c8a-06d1bcda0a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a733fe0e-c999-4c46-a0a5-60f4e8da631f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3284e3d9-3f5f-4ed0-850c-a31a0ee22931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de526b69-9161-430d-ad43-1ecfa4f53289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f99e98-62dc-4e9e-8751-cefaf099dd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78844b3f-c44b-41f3-ae99-5e57769f881e",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e950eaf4-3132-4fc4-9494-29b2a08dff66",
   "metadata": {},
   "source": [
    "![title](top.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df1e107-4fb1-4dda-aad9-6a4375d324c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ffe179-e034-4222-be16-5b6137ef538b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc8d168-2bde-485e-98cb-166317ede3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f42cb-c9e8-467d-b626-6e7ef1c28ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6054f32c-586c-4cf0-8643-12922ff3f072",
   "metadata": {},
   "source": [
    "# Парсинг статьи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164df75-474f-40c6-ae22-bb557a76b2e0",
   "metadata": {},
   "source": [
    "![title](deepseek.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc52669-7f84-4767-ad9c-892976ce4d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d11432a-2509-4d91-9bcf-4220853722b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51018db4-3cd3-4ee2-9dc9-b8fab8a09f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6c7697-2b5c-47c4-a543-e176ec32193d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "757b9c17-15fc-47dc-8639-665af892570c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is group relative policy optimization\n",
      "Group Relative Policy Optimization (GRPPO) is not a widely recognized term in the field of reinforcement learning or machine learning. It's possible that there might be a mix-up with other related concepts, or it could be a custom or specialized term used in a specific context.\n",
      "\n",
      "However, based on common reinforcement learning techniques and terminology, we can infer some possibilities:\n",
      "\n",
      "1. **Relative Policy Optimization**: This concept typically refers to methods where agents optimize their policies relative to others. For example, in multi-agent settings, an agent might learn its policy by comparing itself to other agents.\n",
      "\n",
      "2. **Group Policy Optimization**: This could refer to optimizing a policy for a group of agents, rather than an individual agent. It might involve coordinating actions across multiple agents to achieve a collective goal.\n",
      "\n",
      "3. **Policy Gradient Methods**: These are a class of reinforcement learning algorithms that optimize policies directly using gradient ascent. GRPPO could be a variant of such methods.\n",
      "\n",
      "Without more context, it's challenging to provide a precise definition. If you have more information about the specific application or context in which this term is used, it would help in providing a more accurate explanation.\n",
      "--------------------------------------------------\n",
      "What rewards are used in DeepSeek-R1-Zero\n",
      "I don't have specific information about the exact rewards used in \"DeepSeek-R1-Zero\" as it sounds like a hypothetical or fictional context, possibly related to a research project or game. However, I can provide some general ideas of what types of rewards might be used in such an AI training scenario:\n",
      "\n",
      "Some common reward functions for reinforcement learning tasks include:\n",
      "\n",
      "1. Simple binary rewards (e.g., +1 for goal achieved, -1 for failure)\n",
      "2. Continuous rewards based on proximity to the goal\n",
      "3. Reward shaping to encourage specific behaviors\n",
      "4. Negative penalties for undesirable actions\n",
      "5. Exploration bonuses to encourage trying new things\n",
      "\n",
      "Without more specific details about this particular system, it's difficult to say exactly what rewards are being used. If you have more information about the context or objectives of this system, I'd be happy to try providing more targeted insights.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "questions = [\"What is group relative policy optimization\", \"What rewards are used in DeepSeek-R1-Zero\"]\n",
    "\n",
    "for question in questions:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    print(question)\n",
    "    print(ask_local_llm(messages))\n",
    "    print(\"-----\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1194fce0-6be2-4f92-bbea-6695fbb671e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56851\n",
      "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\n",
      "Reinforcement Learning\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\n",
      "DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without su\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "def pdf_to_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    for page in doc:\n",
    "        full_text += page.get_text()\n",
    "    return full_text\n",
    "\n",
    "# Example usage\n",
    "text = pdf_to_text(\"/root/2501.12948v1.pdf\")\n",
    "print(len(text))\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dc370c-0bdf-47a3-be60-c9da48221848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a92ca8-72c8-4ac1-9767-f61258d5e9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff0d62-ad41-401f-9a6c-8ab4316a3372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c4184b-f60a-43d0-a931-735d9190b90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3ded65-850c-4fd2-878b-f17fb143aa31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d7e1bd-2e31-47a4-aa39-425f0d4aa046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475bd82c-6825-4591-ba47-a35745af073f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29527801-7bcc-495f-8bb6-2593417b9e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf27bfca-6fd9-4d07-b710-8381f9bf2841",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab043115-3cef-4bce-a52d-7c41854876f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "length = 800\n",
    "for start in range(0, len(text), 500):\n",
    "    chunks.append(text[start: start + length])\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19785b7b-fa82-40c3-ab13-d8efb48e83ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55763252-a86e-4d40-8211-dbf5698cc22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71613d-ee84-418c-bfd3-a9ddbb929f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a14436c-ef58-49b6-bf2d-9a151baab2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28c7d0-7b86-4510-a5ef-07b97833e36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64249fa2-d05b-456d-bf28-ae48e0046bb9",
   "metadata": {},
   "source": [
    "**перекрытие по предложениям**\n",
    "\n",
    "Chunk 1:\n",
    "* Marie Curie was a pioneering physicist and chemist who conducted groundbreaking research on radioactivity. She was the first woman to win a Nobel Prize.\n",
    "\n",
    "Chunk 2:\n",
    "* She was the first woman to win a Nobel Prize. She is the only person to win Nobel Prizes in two different scientific fields.\n",
    "\n",
    "Chunk 3:\n",
    "* She is the only person to win Nobel Prizes in two different scientific fields. Her discoveries included the elements polonium and radium, which she isolated from pitchblende."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e37295-7ccb-409b-8d01-b59024287bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726c1290-f867-440d-8191-56f56f2c32eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9374fa9-4290-4181-aa4f-d6eda991ef6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505a737-6194-4869-b840-9a64f828b911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ba3b30-f1b5-469d-93fc-2077afa672f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d86c10e1-e576-48ad-bd9e-55c11bb5ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " General Intelligence (AGI).\n",
      "Recently, post-training has emerged as an important component of the full training pipeline.\n",
      "It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt\n",
      "to user preferences, all while requiring relatively minimal computational resources against\n",
      "pre-training. In the context of reasoning capabilities, OpenAI’s o1 (OpenAI, 2024b) series models\n",
      "were the first to introduce inference-time scaling by increasing the length of the Chain-of-\n",
      "Thought reasoning process. This approach has achieved significant improvements in various\n",
      "reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge\n",
      "of effective test-time scaling remains an open question for the research community. Several prior\n",
      "works have explore\n",
      "\n",
      "-------\n",
      "\n",
      "hought reasoning process. This approach has achieved significant improvements in various\n",
      "reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge\n",
      "of effective test-time scaling remains an open question for the research community. Several prior\n",
      "works have explored various approaches, including process-based reward models (Lightman\n",
      "et al., 2023; Uesato et al., 2022; Wang et al., 2023), reinforcement learning (Kumar et al., 2024),\n",
      "and search algorithms such as Monte Carlo Tree Search and Beam Search (Feng et al., 2024; Trinh\n",
      "et al., 2024; Xin et al., 2024). However, none of these methods has achieved general reasoning\n",
      "performance comparable to OpenAI’s o1 series models.\n",
      "In this paper, we take the first step toward improving language model reasoning capabi\n"
     ]
    }
   ],
   "source": [
    "print(chunks[7])\n",
    "print(\"\\n-------\\n\")\n",
    "print(chunks[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ed047-4afc-4c79-b0ff-84cfd92fe8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9afa1e9-cb80-4fbc-9f75-043384f65e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e430e2bd-4677-4286-a3c6-877a49b28b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3940f76-3643-43f8-8ff7-5b0454bcf0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e9b03-6749-4f59-bf18-7dfcd3d1715f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8bcd24-ace1-428d-9dd4-4387af5da774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9213099-7ce2-42c6-9522-d0e6b604eecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4149dedd-be69-420a-8e13-08c2fd15ba4c",
   "metadata": {},
   "source": [
    "# Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0305434-c434-4380-87d9-84b4eb3085ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'group', 'relative', 'policy', 'optimization']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    return text.lower().split()\n",
    "\n",
    "print(tokenize(questions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb5071e8-e5ad-46ba-8393-bb5bf70562c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def score_chunk(question_tokens: List[str], chunk_tokens: List[str]):\n",
    "    question_count = Counter(question_tokens)\n",
    "    chunk_count = Counter(chunk_tokens)\n",
    "    overlap = sum(min(question_count[word], chunk_count[word]) for word in question_count)\n",
    "    return overlap\n",
    "\n",
    "print(score_chunk(tokenize(\"How far is the sun\"), tokenize(\"The sun is very far\")))\n",
    "print(score_chunk(tokenize(\"How far is the sun\"), tokenize(\"The moon is close\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90bf69f1-f691-4a2e-ad45-d0a156af5072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_chunk(question: str, chunks: List[str]):\n",
    "    question_tokens = tokenize(question)\n",
    "    best_score = 0\n",
    "    best_chunk = None\n",
    "    for chunk in chunks:\n",
    "        chunk_tokens = tokenize(chunk)\n",
    "        score = score_chunk(question_tokens, chunk_tokens)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_chunk = chunk\n",
    "    return best_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62507929-8c2a-43b6-860a-264296f582aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is group relative policy optimization\n",
      "iting results, andhope this provides the community with valuable insights.2.2.1. Reinforcement Learning AlgorithmGroup Relative Policy OptimizationIn order to save the training costs of RL, we adopt GroupRelative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that istypically the same size as the policy model, and estimates the baseline from group scores instead.Specifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1, 𝑜2, · · · , 𝑜𝐺} from the oldpolicy 𝜋𝜃𝑜𝑙𝑑and then optimizes the policy model 𝜋𝜃by maximizing the following objective:J𝐺𝑅𝑃𝑂(𝜃) = E[𝑞∼𝑃(𝑄), {𝑜𝑖}𝐺𝑖=1 ∼𝜋𝜃𝑜𝑙𝑑(𝑂|𝑞)]1𝐺𝐺∑︁𝑖=1\u0012min\u0012 𝜋𝜃(𝑜𝑖|𝑞)𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞) 𝐴𝑖, clip\u0012 𝜋𝜃(𝑜𝑖|𝑞)𝜋𝜃𝑜𝑙𝑑(𝑜𝑖|𝑞) , 1 −𝜀, 1 + 𝜀\u0013𝐴𝑖\u0013−𝛽D𝐾𝐿\u0000𝜋𝜃||𝜋𝑟𝑒𝑓\u0001\u0013,(1)D𝐾𝐿\u0000𝜋𝜃||𝜋𝑟𝑒𝑓\u0001 =𝜋𝑟𝑒𝑓(𝑜𝑖|𝑞)𝜋𝜃(𝑜𝑖|𝑞) −lo\n"
     ]
    }
   ],
   "source": [
    "print(questions[0])\n",
    "print(find_best_chunk(questions[0], chunks).replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "496dfaeb-4e17-4eba-a32a-f973546f15a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What rewards are used in DeepSeek-R1-Zero\n",
      "rs.\n",
      "In this work, we collect thousands of cold-start data to fine-tune the DeepSeek-V3-Base as\n",
      "the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data\n",
      "9\n",
      "include:\n",
      "• Readability: A key limitation of DeepSeek-R1-Zero is that its content is often not suitable\n",
      "for reading. Responses may mix multiple languages or lack markdown formatting to\n",
      "highlight answers for users. In contrast, when creating cold-start data for DeepSeek-R1,\n",
      "we design a readable pattern that includes a summary at the end of each response and\n",
      "filters out responses that are not reader-friendly. Here, we define the output format as\n",
      "|special_token|<reasoning_process>|special_token|<summary>, where the reasoning\n",
      "process is the CoT for the query, and the summary is used to summarize the reasoning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(questions[1])\n",
    "print(find_best_chunk(questions[1], chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa401e-becd-4303-b0eb-a6e06a251733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac66b4fa-f7be-4aaf-920f-3851011cda80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02051f-838f-464e-894b-5a071c1e3879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f87b72-c0b3-48a4-8371-1953337d1072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18168bb1-852f-4430-8676-ac83a9e81758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a26a5b1-b239-4df1-8cca-428c2f4a90b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3023b3-6948-4d1d-80bd-020784ec242b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a42519c-1f3d-4189-9982-a2b0d3e43897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f25843-4ecd-4af0-a14f-dd8a42f6eca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77f7b1-573b-40f8-8056-f4601de5ee5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18a6e548-2bd2-43c5-a9db-7461e4320527",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72245ed-3b31-4ae3-b90f-3781d4307dae",
   "metadata": {},
   "source": [
    "![title](rag.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f86f076e-20f2-4c39-8801-83d327b59eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-en-v1.5')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-large-en-v1.5').cuda()\n",
    "model = model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "432f47f5-0ab8-4e07-adaf-c02f7f3061fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_text_embeddings(model, tokenizer, texts: List[str]):\n",
    "    device = next(model.parameters()).device\n",
    "    encoded_input = tokenizer(texts, padding=\"longest\", truncation=True, return_tensors='pt')\n",
    "    for k, v in encoded_input.items():\n",
    "        encoded_input[k] = v.to(device)\n",
    "    model_output = model(**encoded_input)\n",
    "    sentence_embeddings = model_output[0][:, 0]\n",
    "    sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aec909-7d38-44f6-b18b-1933c9a5d4da",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| |\\mathbf{b}| \\cos\\theta = a_1b_1 + a_2b_2 + \\dots + a_n b_n = \\sum_{i=1}^n a_i b_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1e35d48-e9ef-4b01-b4d7-5aa197f5183d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0252,  0.0195, -0.0036,  ...,  0.0125,  0.0009,  0.0136],\n",
      "        [ 0.0424,  0.0380, -0.0261,  ...,  0.0123,  0.0155,  0.0342]],\n",
      "       device='cuda:0')\n",
      "torch.Size([2, 1024])\n"
     ]
    }
   ],
   "source": [
    "question_embeddings = get_text_embeddings(model, tokenizer, questions)\n",
    "print(question_embeddings)\n",
    "print(question_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4192d89-e050-4025-bce6-6445268a1930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.5336],\n",
       "        [0.5336, 1.0000]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_embeddings @ question_embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63e4aee7-4cac-41cf-8773-750cc3a6f630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:00<00:00, 188.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([114, 1024])\n",
      "114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "batch = []\n",
    "chunk_embeddings = []\n",
    "from tqdm import tqdm\n",
    "for chunk in tqdm(chunks):\n",
    "    batch.append(chunk)\n",
    "    if len(batch) % batch_size == 0:\n",
    "        chunk_embeddings.append(get_text_embeddings(model, tokenizer, batch))\n",
    "        batch = []\n",
    "\n",
    "if len(batch):\n",
    "    chunk_embeddings.append(get_text_embeddings(model, tokenizer, batch))\n",
    "\n",
    "chunk_embeddings = torch.cat(chunk_embeddings, dim=0)\n",
    "print(chunk_embeddings.shape)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6540b14-5932-421f-8d96-a2b6483d6a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4980e89f-e7a6-402d-9aa7-6b5d3a14e787",
   "metadata": {},
   "source": [
    "# Векторный поиск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "573d9497-9fe9-42c5-b964-b20ee67d9e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What rewards are used in DeepSeek-R1-Zero\n"
     ]
    }
   ],
   "source": [
    "print(questions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55c93145-516a-42e5-8db5-f3cc5b6f3d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7398, 0.7156, 0.6572, 0.7738, 0.7478, 0.6827, 0.5816, 0.5162, 0.6219,\n",
      "         0.7068, 0.7227, 0.7463, 0.7479, 0.7089, 0.7175, 0.7692, 0.6546, 0.7096,\n",
      "         0.7270, 0.6583, 0.6993, 0.6795, 0.7223, 0.7504, 0.5697, 0.5908, 0.5930,\n",
      "         0.7554, 0.8046, 0.7135, 0.6966, 0.6823, 0.7075, 0.7034, 0.6882, 0.7386,\n",
      "         0.7285, 0.7340, 0.6470, 0.6853, 0.6542, 0.6000, 0.5664, 0.7149, 0.7342,\n",
      "         0.6951, 0.7371, 0.7466, 0.6890, 0.6906, 0.5979, 0.5920, 0.6481, 0.6766,\n",
      "         0.6860, 0.7588, 0.7855, 0.6566, 0.6535, 0.6023, 0.5545, 0.6184, 0.6795,\n",
      "         0.6005, 0.6673, 0.6563, 0.6483, 0.6854, 0.5122, 0.6249, 0.6491, 0.6149,\n",
      "         0.6643, 0.6554, 0.6643, 0.6850, 0.6880, 0.7513, 0.6889, 0.6842, 0.7004,\n",
      "         0.7198, 0.5949, 0.6409, 0.5872, 0.5980, 0.5712, 0.7292, 0.7643, 0.7097,\n",
      "         0.6914, 0.6865, 0.6387, 0.4562, 0.5737, 0.6438, 0.6169, 0.5382, 0.5454,\n",
      "         0.5876, 0.5444, 0.5494, 0.5430, 0.6046, 0.5877, 0.5307, 0.5501, 0.5844,\n",
      "         0.6676, 0.4982, 0.4667, 0.4614, 0.4081, 0.4145]], device='cuda:0')\n",
      "\n",
      "torch.return_types.topk(\n",
      "values=tensor([[0.8046, 0.7855, 0.7738, 0.7692, 0.7643, 0.7588, 0.7554, 0.7513, 0.7504,\n",
      "         0.7479]], device='cuda:0'),\n",
      "indices=tensor([[28, 56,  3, 15, 88, 55, 27, 77, 23, 12]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "q = [questions[1]]\n",
    "q_emb = get_text_embeddings(model, tokenizer, q)\n",
    "score = q_emb @ chunk_embeddings.T\n",
    "print(score)\n",
    "print()\n",
    "print(score.topk(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c90aac-069a-490c-a145-b1dc03d6383c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac4d5a04-0ad8-423f-8307-49fdb0ce8eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What rewards are used in DeepSeek-R1-Zero\n",
      "te for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning\n",
      "question during training.\n",
      "2.2.2. Reward Modeling\n",
      "The reward is the source of the training signal, which decides the optimization direction of RL.\n",
      "To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two\n",
      "types of rewards:\n",
      "• Accuracy rewards: The accuracy reward model evaluates whether the response is correct.\n",
      "For example, in the case of math problems with deterministic results, the model is required\n",
      "to provide the final answer in a specified format (e.g., within a box), enabling reliable\n",
      "rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be\n",
      "used to generate feedback based on predefined test cases.\n",
      "• Format rewards: In addition to the accuracy r\n"
     ]
    }
   ],
   "source": [
    "print(questions[1])\n",
    "print(chunks[28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92b09cd5-bd8f-495d-85bc-9a6cd42d21ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In DeepSeek-R1-Zero, the reward system primarily consists of two types of rewards:\n",
      "\n",
      "1. **Accuracy Rewards**: These rewards evaluate whether the generated response is correct. For deterministic problems like math problems, the model needs to provide the final answer in a specified format (e.g., within a box) to enable reliable verification of correctness. For problems like those found on LeetCode, a compiler can generate feedback based on predefined test cases.\n",
      "\n",
      "2. **Format Rewards**: These rewards ensure that the response adheres to the required format or structure. This could include checking if the response is in the correct format, such as a specific text box for answers, or ensuring that the output is structured correctly according to the problem's requirements.\n",
      "\n",
      "These rewards guide the model towards generating responses that are both accurate and formatted correctly, thereby optimizing its performance on the given tasks.\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "question = questions[1]\n",
    "question_embedding = get_text_embeddings(model, tokenizer, [question])\n",
    "scores = question_embeddings @ chunk_embeddings.T\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": chunks[score.topk(1).indices[0][0]]},\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "]\n",
    "print(ask_local_llm(messages))\n",
    "print(\"\\n\\n\")\n",
    "print(\"--------\" * 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b86d1-f66b-4370-b2b6-e9befa2c70dc",
   "metadata": {},
   "source": [
    "# Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34092b3c-da5e-4a85-b921-8c49f5e0122c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokenizer_reranker = AutoTokenizer.from_pretrained('BAAI/bge-reranker-base')\n",
    "reranker = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base').cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "864389fd-8c89-4953-9aca-2c5f4677f6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8.1544,  6.1821], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_pair_scores(reranker, tokenizer, pairs):\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = v.cuda()\n",
    "    scores = reranker(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "    return scores\n",
    "\n",
    "\n",
    "pairs = [['what is panda?', 'hi'], \n",
    "         ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]\n",
    "get_pair_scores(reranker, tokenizer_reranker, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8723472c-a1f1-41d7-b779-27303799babb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 56, 3, 15, 88, 55, 27, 77, 23, 12]\n"
     ]
    }
   ],
   "source": [
    "q = [questions[1]]\n",
    "q_emb = get_text_embeddings(model, tokenizer, q)\n",
    "score = q_emb @ chunk_embeddings.T\n",
    "top_k_indices = score.topk(10).indices[0].tolist()\n",
    "print(top_k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c052a119-1ac7-4c28-9f6c-a191740a8fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-10.1965, -10.1962,  -8.6665, -10.1968, -10.1949, -10.1968, -10.1962,\n",
      "        -10.1968,  -8.6665,  -8.6665], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    [question, chunk[idx]]\n",
    "    for idx in top_k_indices\n",
    "]\n",
    "scores = get_pair_scores(reranker, tokenizer_reranker, pairs)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63814733-5ff2-4019-b0cc-079930676dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 2, 9, 4, 1, 6, 0, 7, 3, 5], device='cuda:0')\n",
      "[23, 3, 12, 88, 56, 27, 28, 77, 15, 55]\n"
     ]
    }
   ],
   "source": [
    "argsort = scores.argsort(descending=True)\n",
    "print(argsort)\n",
    "sorted_indices = [top_k_indices[idx] for idx in argsort]\n",
    "print(sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bf0df1-dcd5-4ebc-9e23-41c998408664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90953067-62c9-45a6-8192-d56fb7dd32ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8b7e5-7c98-47e1-baca-c8b3944de362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a5720-90cf-4689-81e0-86f6921e8cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa56d5-ea39-40e3-a72f-fb125731f67f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e83f94-d9ec-483b-995b-17510c0aef1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fd67512-2720-4c7d-9dff-ecdab0dbbf3d",
   "metadata": {},
   "source": [
    "![title](top.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f5559a-bf9d-4bbf-a1dc-2a111e521809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508bb90-c233-4bbb-a446-4dc5f126e865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c209e7-e400-490f-a6a6-a01081df97bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
