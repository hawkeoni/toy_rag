{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17097f0d-dd05-4fa2-a27b-f5f5a397414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f86f076e-20f2-4c39-8801-83d327b59eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-en-v1.5')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-large-en-v1.5').cuda()\n",
    "model = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b147fc3-f943-4654-b26e-5d448c17bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks, embeds = torch.load(\"database.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aadab346-1aef-4435-a3a4-139fc1562a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is group relative policy optimization\n",
      " Group relative policy optimization (GRPO) is an approach designed to address the high computational cost and complexity associated with training traditional reinforcement learning (RL) algorithms, namely actor-critic methods. These methods typically involve maintaining a critic model that is on par in size with the policy model, making each training iteration expensive and time-consuming.\n",
      "\n",
      "To cut down on training costs, GRPO adopts an innovative strategy. It samples a group of outputs (answer records) â€” denoted as {ğ‘œ1, ğ‘œ2, Â· Â· Â· , ğ‘œğº} â€” from the old policy model denoted by ğœƒğœ‘ğ’¯ğ’¸ and optimizes the policy model ğœ‹ğœƒ by maximizing the following objective:\n",
      "\n",
      "Jğºğ‘…ğ‘ƒğ‘‚(ğœƒ) = E[ğ‘âˆ¼ğ‘ƒ(ğ‘„), {ğ‘œğ‘–}ğº\n",
      "ğ‘–=1 âˆ¼ğœ‹ğœƒğ‘œğ‘™ğ‘‘(ğ‘‚|ğ‘„)]\n",
      "\n",
      "In this expression, \n",
      "ğœƒ represents parameters of ğœ‘ğœ‘ğœ‘ğœ‘ğ‘ƒ\n",
      "ğ‘„ represents labels (questions)\n",
      "ğ‘‚ represents output records (responses)\n",
      "ğ‘ˆğ‘ represents utterances\n",
      "ğ‘– represents the iteration in sampling from ğ‘‹ğ‘Œ = {ğ‘‚1, ğ‘‚2, ... , ğ‘‚ğº}\n",
      "ğœ‹ğœƒğ‘œğ‘™ğ‘‘ denotes the policy model and outputs a distribution over tasks based on the question ğ‘„.\n",
      "ğœ‹ğœ‘ğœ‘ğœ‘ğ‘ƒ(ğ‘‚|ğ‘„,ğ‘˜) denotes the old policy model and outputs a distribution over tasks each time a question ğ‘„ is presented based on the old model ğœ‘ğœ‘ğœ‘ğœ‘ğ‘ƒ \n",
      "If we have sample subsets containing ğ‘†ğ‘…ğ‘Ÿğ‘’ğ‘” and ğ‘…ğ‘’ğ‘â„ğ‘’ğ‘Ÿğ‘œğ‘¢ğ‘  over a question ğ‘„ (Q), the objective function Jğºğ‘…ğ‘ƒğ‘‚(ğœƒ) aims to maximize (1 - ğœ€)*|ğ‘†ğ‘…ğ‘Ÿğ‘’ğ‘”| + ğœ€*|ğ‘…ğ‘’ğ‘â„ğ‘’ğ‘Ÿğ‘œğ‘¢ğ‘ |, where ğœ€ is a hyperparameter within the range [0, 1].\n",
      "\n",
      "By operating directly on a group of task scores (sampled by the old model), GRPO trades off accuracy and complexity for reduced computational cost, making training faster and more manageable, while still enabling capabilities similar to off-the-shelf models.\n",
      "\n",
      "In practical terms, GRPO can result in faster convergence over time intervals with a smaller number of batches while maintaining a high degree of performance when compared to traditional actor-critic algorithms. Additionally, GRPO demonstrates resilience against rare events, produces improved accuracy over traditional deep-RL approaches, and performs well even on scenarios with a limited number of samples.\n",
      "\n",
      "To recap, GRPO offers a computationally efficient approach to deep RL training by using group scores instead of critic models for estimating the baseline. This property of GRPO makes it attractive for real-life applications (RFA) that require fast convergence and improved performance, such as zero-shot dialog scenarios.\n",
      "What rewards are used in DeepSeek-R1-Zero\n",
      " <think> Reward Modeling is a key component of training DeepSeek-R1-Zero, as it provides the necessary feedback for the algorithm to optimize its responses. </think>\n",
      "<answer> The DeepSeek-R1-Zero uses a rule-based reward system that comprises of two main types of rewards: Accuracy rewards and reply structure rewards.\n",
      "\n",
      "1. Accuracy Rewards: This reward evaluates the correctness of the response provided by the user. For deterministic problems, such as math, the reward system would prompt the algorithm to provide the final answer in a specific format, such as within a box. \n",
      "\n",
      "2. Reply Structure Rewards: This reward model ensures that the response's structure matches that of a human-like conversation. The system checks whether the generated response is well-formed, contextually appropriate, and maintains coherence with the given conversation history. Violations of structure, such as inappropriate responses or grammatical errors, may result in a lower reward.\n"
     ]
    }
   ],
   "source": [
    "questions = [\"What is group relative policy optimization\", \"What rewards are used in DeepSeek-R1-Zero\"]\n",
    "for query in questions:\n",
    "    encoded_input = tokenizer(query, padding=\"longest\", truncation=True, return_tensors='pt')\n",
    "    for k, v in encoded_input.items():\n",
    "        encoded_input[k] = v.cuda()\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        sentence_embeddings = model_output[0][:, 0]\n",
    "        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        topk = (sentence_embeddings @ embeds.T).topk(10)\n",
    "        chunk = chunks[topk.indices[0][0]]\n",
    "        \n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"system\", \"content\": chunk},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    print(query)\n",
    "    print(response.json()[\"choices\"][0][\"message\"][\"content\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61c674-0946-4fc9-8697-f6c41170f797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9525a0c1-bb6a-424b-acf3-46f383495084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.6336, 0.6263, 0.6240, 0.6224, 0.6200, 0.6142, 0.6137, 0.6088, 0.6086,\n",
       "         0.6046]], device='cuda:0'),\n",
       "indices=tensor([[106,  25,   8,  13,  40,  22,  41, 107, 102,  42]], device='cuda:0'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ef787-460f-499c-99db-e2f1178d141c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae48002-91ef-4f92-a339-31a07bf61a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLaDA (Large Language Diffusion Model) is an innovative framework presented for large language modeling. Its unique approach stems from utilizing diffusion models, which are part of a class of generative models that iteratively transform a distribution of noise into a coherent output or data sample. LLaDA stands out because it is the first to apply these diffusion models to natural language processing effectively. \n",
      "\n",
      "\n",
      "The main contributions and features of LLaDA include:\n",
      "\n",
      "\n",
      "1. **Scalability:** LLaDA can be scaled up to handle larger datasets and generate more complex language patterns due to its diffusion-based architecture.\n",
      "\n",
      "\n",
      "2. **In-context Learning:** This denotes the model's ability to understand and generate responses based on the context provided by the preceding text. LLaDA demonstrates this capability quite effectively.\n",
      "\n",
      "\n",
      "3. **Instruction Following:** LLaDA can perform tasks that require understanding complex instructions and generating appropriate responses, challenging the performance of previous models but without the requirement of pre-trained instruction-following models.\n",
      "\n",
      "\n",
      "4. **Bidirectionality:** LLaDA's design enables it to interpret language in both directions, making it a more versatile model when it comes to question answering or conversational AI tasks.\n",
      "\n",
      "\n",
      "5. **Robustness:** LLaDA exhibits increased robustness when compared to traditional autoregressive models (ARMs), withstanding various perturbations and reducing issues such as hallucination (generating plausible-sounding but untrue information).\n",
      "\n",
      "\n",
      "6. **Positive Impact of Biases:** LLaDA's diffusion model framework embraces and isolates biases during the training process, leading to more transparent and understandable model behaviors.\n",
      "\n",
      "\n",
      "Furthermore, studies comparing LLaDA with more conventional autoregressive language models (ARMs) such as GPT-3 show similar or comparable performance, yet offer additional advantages. Notably, these benefits could make diffusion-based models a popular choice for future language models, given their unique properties.\n",
      "\n",
      "\n",
      "However, despite these promising features, there are still limitations due to computational constraints, necessitating ongoing research in this area.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"http://localhost:8000/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"model\": \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"system\", \"content\": chunks[41]},\n",
    "        {\"role\": \"user\", \"content\": \"what is LLaDA?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26429014-b1e3-4d52-ac0f-1c9cad2e140f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f4a1f-f0b0-443e-a443-bab72b4becbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c2020-fc1f-4afe-b5d6-80153f82781c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508bb90-c233-4bbb-a446-4dc5f126e865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c209e7-e400-490f-a6a6-a01081df97bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "048bd730-4819-4e9b-b3c8-f108514eb63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d78f11a1-d80e-4c84-a912-63628588b0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe6c3a86-6e29-418d-9513-b79a67aacc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1512, 1024])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479e885-e847-425e-bd02-b38845439aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "par"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
