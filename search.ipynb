{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17097f0d-dd05-4fa2-a27b-f5f5a397414c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f86f076e-20f2-4c39-8801-83d327b59eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-en-v1.5')\n",
    "model = AutoModel.from_pretrained('BAAI/bge-large-en-v1.5').cuda()\n",
    "model = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b147fc3-f943-4654-b26e-5d448c17bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks, embeds = torch.load(\"database.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aadab346-1aef-4435-a3a4-139fc1562a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is group relative policy optimization\n",
      " Group relative policy optimization (GRPO) is an approach designed to address the high computational cost and complexity associated with training traditional reinforcement learning (RL) algorithms, namely actor-critic methods. These methods typically involve maintaining a critic model that is on par in size with the policy model, making each training iteration expensive and time-consuming.\n",
      "\n",
      "To cut down on training costs, GRPO adopts an innovative strategy. It samples a group of outputs (answer records) — denoted as {𝑜1, 𝑜2, · · · , 𝑜𝐺} — from the old policy model denoted by 𝜃𝜑𝒯𝒸 and optimizes the policy model 𝜋𝜃 by maximizing the following objective:\n",
      "\n",
      "J𝐺𝑅𝑃𝑂(𝜃) = E[𝑞∼𝑃(𝑄), {𝑜𝑖}𝐺\n",
      "𝑖=1 ∼𝜋𝜃𝑜𝑙𝑑(𝑂|𝑄)]\n",
      "\n",
      "In this expression, \n",
      "𝜃 represents parameters of 𝜑𝜑𝜑𝜑𝑃\n",
      "𝑄 represents labels (questions)\n",
      "𝑂 represents output records (responses)\n",
      "𝑈𝑍 represents utterances\n",
      "𝑖 represents the iteration in sampling from 𝑋𝑌 = {𝑂1, 𝑂2, ... , 𝑂𝐺}\n",
      "𝜋𝜃𝑜𝑙𝑑 denotes the policy model and outputs a distribution over tasks based on the question 𝑄.\n",
      "𝜋𝜑𝜑𝜑𝑃(𝑂|𝑄,𝑘) denotes the old policy model and outputs a distribution over tasks each time a question 𝑄 is presented based on the old model 𝜑𝜑𝜑𝜑𝑃 \n",
      "If we have sample subsets containing 𝑆𝑅𝑟𝑒𝑔 and 𝑅𝑒𝑐ℎ𝑒𝑟𝑜𝑢𝑠 over a question 𝑄 (Q), the objective function J𝐺𝑅𝑃𝑂(𝜃) aims to maximize (1 - 𝜀)*|𝑆𝑅𝑟𝑒𝑔| + 𝜀*|𝑅𝑒𝑐ℎ𝑒𝑟𝑜𝑢𝑠|, where 𝜀 is a hyperparameter within the range [0, 1].\n",
      "\n",
      "By operating directly on a group of task scores (sampled by the old model), GRPO trades off accuracy and complexity for reduced computational cost, making training faster and more manageable, while still enabling capabilities similar to off-the-shelf models.\n",
      "\n",
      "In practical terms, GRPO can result in faster convergence over time intervals with a smaller number of batches while maintaining a high degree of performance when compared to traditional actor-critic algorithms. Additionally, GRPO demonstrates resilience against rare events, produces improved accuracy over traditional deep-RL approaches, and performs well even on scenarios with a limited number of samples.\n",
      "\n",
      "To recap, GRPO offers a computationally efficient approach to deep RL training by using group scores instead of critic models for estimating the baseline. This property of GRPO makes it attractive for real-life applications (RFA) that require fast convergence and improved performance, such as zero-shot dialog scenarios.\n",
      "What rewards are used in DeepSeek-R1-Zero\n",
      " <think> Reward Modeling is a key component of training DeepSeek-R1-Zero, as it provides the necessary feedback for the algorithm to optimize its responses. </think>\n",
      "<answer> The DeepSeek-R1-Zero uses a rule-based reward system that comprises of two main types of rewards: Accuracy rewards and reply structure rewards.\n",
      "\n",
      "1. Accuracy Rewards: This reward evaluates the correctness of the response provided by the user. For deterministic problems, such as math, the reward system would prompt the algorithm to provide the final answer in a specific format, such as within a box. \n",
      "\n",
      "2. Reply Structure Rewards: This reward model ensures that the response's structure matches that of a human-like conversation. The system checks whether the generated response is well-formed, contextually appropriate, and maintains coherence with the given conversation history. Violations of structure, such as inappropriate responses or grammatical errors, may result in a lower reward.\n"
     ]
    }
   ],
   "source": [
    "questions = [\"What is group relative policy optimization\", \"What rewards are used in DeepSeek-R1-Zero\"]\n",
    "for query in questions:\n",
    "    encoded_input = tokenizer(query, padding=\"longest\", truncation=True, return_tensors='pt')\n",
    "    for k, v in encoded_input.items():\n",
    "        encoded_input[k] = v.cuda()\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "        sentence_embeddings = model_output[0][:, 0]\n",
    "        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        topk = (sentence_embeddings @ embeds.T).topk(10)\n",
    "        chunk = chunks[topk.indices[0][0]]\n",
    "        \n",
    "    url = \"http://localhost:8000/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"system\", \"content\": chunk},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    print(query)\n",
    "    print(response.json()[\"choices\"][0][\"message\"][\"content\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61c674-0946-4fc9-8697-f6c41170f797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9525a0c1-bb6a-424b-acf3-46f383495084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.6336, 0.6263, 0.6240, 0.6224, 0.6200, 0.6142, 0.6137, 0.6088, 0.6086,\n",
       "         0.6046]], device='cuda:0'),\n",
       "indices=tensor([[106,  25,   8,  13,  40,  22,  41, 107, 102,  42]], device='cuda:0'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7ef787-460f-499c-99db-e2f1178d141c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae48002-91ef-4f92-a339-31a07bf61a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLaDA (Large Language Diffusion Model) is an innovative framework presented for large language modeling. Its unique approach stems from utilizing diffusion models, which are part of a class of generative models that iteratively transform a distribution of noise into a coherent output or data sample. LLaDA stands out because it is the first to apply these diffusion models to natural language processing effectively. \n",
      "\n",
      "\n",
      "The main contributions and features of LLaDA include:\n",
      "\n",
      "\n",
      "1. **Scalability:** LLaDA can be scaled up to handle larger datasets and generate more complex language patterns due to its diffusion-based architecture.\n",
      "\n",
      "\n",
      "2. **In-context Learning:** This denotes the model's ability to understand and generate responses based on the context provided by the preceding text. LLaDA demonstrates this capability quite effectively.\n",
      "\n",
      "\n",
      "3. **Instruction Following:** LLaDA can perform tasks that require understanding complex instructions and generating appropriate responses, challenging the performance of previous models but without the requirement of pre-trained instruction-following models.\n",
      "\n",
      "\n",
      "4. **Bidirectionality:** LLaDA's design enables it to interpret language in both directions, making it a more versatile model when it comes to question answering or conversational AI tasks.\n",
      "\n",
      "\n",
      "5. **Robustness:** LLaDA exhibits increased robustness when compared to traditional autoregressive models (ARMs), withstanding various perturbations and reducing issues such as hallucination (generating plausible-sounding but untrue information).\n",
      "\n",
      "\n",
      "6. **Positive Impact of Biases:** LLaDA's diffusion model framework embraces and isolates biases during the training process, leading to more transparent and understandable model behaviors.\n",
      "\n",
      "\n",
      "Furthermore, studies comparing LLaDA with more conventional autoregressive language models (ARMs) such as GPT-3 show similar or comparable performance, yet offer additional advantages. Notably, these benefits could make diffusion-based models a popular choice for future language models, given their unique properties.\n",
      "\n",
      "\n",
      "However, despite these promising features, there are still limitations due to computational constraints, necessitating ongoing research in this area.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"http://localhost:8000/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "data = {\n",
    "    \"model\": \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"system\", \"content\": chunks[41]},\n",
    "        {\"role\": \"user\", \"content\": \"what is LLaDA?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26429014-b1e3-4d52-ac0f-1c9cad2e140f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f4a1f-f0b0-443e-a443-bab72b4becbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c2020-fc1f-4afe-b5d6-80153f82781c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508bb90-c233-4bbb-a446-4dc5f126e865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c209e7-e400-490f-a6a6-a01081df97bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "048bd730-4819-4e9b-b3c8-f108514eb63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d78f11a1-d80e-4c84-a912-63628588b0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe6c3a86-6e29-418d-9513-b79a67aacc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1512, 1024])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479e885-e847-425e-bd02-b38845439aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "par"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
